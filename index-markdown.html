<div data-align="left">
<p>– –</p>
</div>
<p>– –</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;">Nikhil R Devanur</td>
</tr>
</tbody>
</table>
<p>– –</p>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: center;"><img src="files/Profile2019.jpg" /></td>
<td><p>– –</p>
<p>Bio: Nikhil R. Devanur is a Principal Scientist at Amazon working on <a href="https://advertis%20ing.amazon.com/products/sponsored%20-products/?ref_=a20m_us_gw_splm">Sponsored Products</a>. Previously, he was the manager of the <a href="h%20ttps://www.microsoft.com/en-us/re%20search/group/algorithms-redmond/">Algorithms group</a> in [Microsoft Research] (http://research.microsoft.com/), Redmond.</p>
<p>He is interested in what he calls Automated Economics, which studies the question of how technology can be used to improve the efficiency of economic systems. His other interest is in Algorithms: he is interested in designing algorithms that are faster, simpler, work online or in a distributed fashion, for some of the basic combinatorial optimization problems.</p>
<p>Contact: Iam@nikhildevanur.com</p>
<p>– –</p>
<p>Mentees (Interns)</p>
<ul>
<li>Ben Birnbaum, Flatiron Healrh</li>
<li><a href="htt%20p://pages.cs.wisc.edu/~balu2901/">Balu Sivan,</a> Google Research</li>
<li><a href="http://i.cs.hku.hk/~zhiyi/">Zhiyi Hu ang,</a> University of Hong Kong</li>
<li><a href="http://jamiemorgenstern.com/">Jamie Morgenster n,</a> Georgia Tech</li>
<li>[Janardhan</li>
</ul>
<p>Kulkarni,](https://www.microsoft. com/en-us/research/people/jakul/) Microsoft Research - [Alex Psomas,] (http://www.cs.cmu.edu/~cpsomas/) CMU - [Rad Niazadeh,] (http://www.cs.cornell.edu/~rad/) Stanford - <a href="htt%20p://www.cc.gatech.edu/~syazdanb/">Sadra Yazdanbod,</a> Google - <a href="http://ho%20mes.cs.washington.edu/~kgoldner/">Kira Goldner,</a> University of Washington - <a href="htt%20p://www.cs.cornell.edu/~teddlyk/">Thodoris Lykouris,</a> Cornell University - <a href="http://jakub.tarnawski.org/">Jakub Tarnaws ki,</a> EPFL</p></td>
</tr>
</tbody>
</table>
<div data-align="left">
<p>– –</p>
<p><span id="pubselect">News</span></p>
</div>
<ul>
<li>Fall 2019: I moved to Amazon as a Principal Scientist. I will be part of the <a href="https://advertising.amazon.com/products/sponsored-products/?ref_=a20m_us_gw_splm">Sponsored Products</a> team.</li>
<li>Spring 2019: We are hiring! Please <a href="%20https://www.microsoft.com/en-us/research/group/algorithms-redmond/#!opportunities">apply</a>.</li>
<li>Spring 2017: I now manage the newly created <a href="https://www.microsoft.com/en-us/research/group/algorithms-redmond/">Algorithms group</a> in Micrsoft Research, Redmond.</li>
<li>I am co-teaching (with Anna Karlin) a <a href="https://courses.cs.washington.edu/courses/cse522/17sp/">course on Algorithms and Uncertainty</a> at UW this spring quarter, 2017.</li>
<li>I am the co-Program Committee Chair of <a href="http://lcm.csa.iisc.ernet.in/wine2017/">WINE 2017.</a></li>
<li>I am organizing an <a href="http://wecc.azurewebsites.net/">ACM EC Workshop on Economic Aspects of Cloud Computing</a>. Submission deadline is May 17th, 2016.</li>
<li>I am the Program Committee Chair of <a href="http://cui.unige.ch/tcs/random-approx/2014/index.php">APPROX 2014</a> .</li>
<li>I organized a <a href="https://sites.google.com/site/onlinematchingstoc/home">workshop on Online matching at STOC</a> in Palo Alto on June 1st 2013.</li>
<li>I gave a tutorial on <a href="http://www.sigecom.org/ec13/schedule_tutorials.html">Prior-Robust Optimization at EC</a> in Philadelphia on June 16 2013.</li>
<li>I taught a <a href="OnlineAlgoCourse/index1.html">course on online algorithms</a> at UW this winter quarter, 2013.</li>
</ul>
<div data-align="left">
<p>– –</p>
<p><span id="pubselect">Manuscripts</span></p>
</div>
<ul>
<li><strong>A new auction format for IPL players auctions</strong> [<a href="pubs/IPLDraftAuctionsv3.1.pdf">pdf</a>].</li>
<li><a href="http://www.informatik.uni-trier.de/~ley/pers/hd/d/Devanur:Nikhil_R=">DBLP page</a></li>
<li><a href="https://arxiv.org/search/?query=nikhil+devanur&amp;searchtype=author&amp;abstracts=hide&amp;order=-announced_date_first&amp;size=50">Papers on Arxiv</a></li>
</ul>
<div data-align="left">
<p>– –</p>
<p><span id="pubselect">Selected Recent Publications</span></p>
</div>
<p><strong>2021</strong></p>
<ul>
<li><strong>Proportional Dynamics in Exchange Economies.</strong><br />
with Simina Br^anzei, and Yuval Rabani. In Proc. of <em>ACM EC 2021</em>.</li>
<li><strong>Designing a Combinatorial Financial Options Market.</strong><br />
with Xintong Wang, David M. Pennock, David M. Rothschild, Biaoshuai Tao, and Michael P. Wellman. In Proc. of <em>ACM EC 2021</em>.</li>
<li><strong>Static pricing for multi-unit prophet inequalities.</strong><br />
with Shuchi Chawla, and Thodoris Lykouris. In Proc. of <em>WINE 2021</em>.</li>
</ul>
<p><strong>2020</strong></p>
<ul>
<li><strong>Blink: Fast and Generic Collectives for Distributed ML.</strong><br />
with Guanhua Wang, Shivaram Venkataraman, Amar Phanishayee, Jorgen Thelin, and Ion Stoica. In Proc. of <em>MLSys 2020</em>.</li>
<li><strong>Efficient Algorithms for Device Placement of DNN Graph Operators.</strong><br />
with Jakub Tarnawski, Amar Phanishayee, Divya Mahajan, and Fanny Nina Paravecino. In Proc. of <em>NeurIPS 2020</em>.</li>
<li><strong>Optimal Mechanism Design for Single-Minded Agents.</strong><br />
with Kira Goldner, Raghuvansh R. Saxena, Ariel Schvartzman, and S. Matthew Weinberg. In Proc. of <em>ACM EC 2020</em>.</li>
<li><strong>Algorithmic Price Discrimination.</strong><br />
with Rachel Cummings, Zhiyi Huang, and Xiangning Wang. In Proc. of <em>SODA 2020</em>.</li>
</ul>
<p><strong>2019</strong></p>
<ul>
<li><strong>Simple and Approximately Optimal Pricing for Proportional Complementarities.</strong> [<a href="https://arxiv.org/pdf/1909.00788">arXiv pdf</a>| <a href="#PC">Show/Hide Abstract</a>].<br />
with Yang Cai, Kira Goldner, and R. Preston McAfee. In Proc. <em>EC 2019</em>. ::: {#absPC style=“display:none”} We study a new model of complementary valuations, which we call "proportional complementarities.'' In contrast to common models, such as hypergraphic valuations, in our model, we do not assume that the extra value derived from owning a set of items is independent of the buyer's base valuations for the items. Instead, we model the complementarities as proportional to the buyer's base valuations, and these proportionalities are known market parameters. Our goal is to design a simple pricing scheme that, for a single buyer with proportional complementarities, yields approximately optimal revenue. We define a new class of mechanisms where some number of items are given away for free, and the remaining items are sold separately at inflated prices. We find that the better of such a mechanism and selling the grand bundle earns a 12-approximation to the optimal revenue for pairwise proportional complementarities. This confirms the intuition that items should not be sold completely separately in the presence of complementarities. In the more general case, a buyer has a maximum of proportional positive hypergraphic valuations, where a hyperedge in a given hypergraph describes the boost to the buyer's value for item i given by owning any set of items T in addition. The maximum-out-degree of such a hypergraph is d, and k is the positive rank of the hypergraph. For valuations given by these parameters, our simple pricing scheme is an O(min{d,k})-approximation. :::</li>
<li><strong>PipeDream: Generalized Pipeline Parallelism for DNN Training</strong> [<a href="#PPD">Show/Hide Abstract</a>].<br />
with Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri,,Greg Ganger, Phil Gibbons, Matei Zaharia. In Proc. <em>SOSP 2019</em>. ::: {#absPPD style=“display:none”} DNN training is extremely time-consuming, necessitating efficient multi-accelerator parallelization. Current approaches to paralleliz- ing training primarily use intra-batch parallelization, where a single iteration of training is split over the available workers, but suffer from diminishing returns at higher worker counts. We present PipeDream, a system that adds inter-batch pipelining to intra-batch parallelism to further improve parallel training throughput, help- ing to better overlap computation with communication and reduce the amount of communication when possible. PipeDream versions model parameters for backward pass correctness, schedules for- ward and backward passes of different minibatches concurrently on different workers to keep workers well utilized, and systemati- cally partitions DNN layers among workers to balance work and minimize communication. Extensive experimentation with a range of DNN tasks, models, and hardware configurations shows that PipeDream trains models to high accuracy up to 5.28x faster than commonly used inter-batch parallelism techniques. :::</li>
</ul>
<p><strong>2018</strong></p>
<ul>
<li><strong>A New Class of Combinatorial Markets with Covering Constraints: Algorithms and Applications</strong> [<a href="https://arxiv.org/pdf/1511.08748.pdf">arXiv pdf</a>| <a href="#CME">Show/Hide Abstract</a>].<br />
with Jugal Garg, Ruta Mehta, Vijay V. Vazirani, Sadra Yazdanbod. In Proc. <em>SODA 2018</em>. ::: {#absCME style=“display:none”} We introduce a new class of combinatorial markets in which agents have covering constraints over resources required and are interested in delay minimization. Our market model is applicable to several settings including scheduling, cloud computing, and communicating over a network. This model is quite different from the traditional models, to the extent that neither do the classical equilibrium existence results seem to apply to it nor do any of the efficient algorithmic techniques developed to compute equilibria seem to apply directly. We give a proof of existence of equilibrium and a polynomial time algorithm for finding one, drawing heavily on techniques from LP duality and submodular minimization. We observe that in our market model, the set of equilibrium prices could be a connected, non-convex set. To the best of our knowledge, this is the first natural example of the phenomenon where the set of solutions could have such complicated structure, yet there is a combinatorial polynomial time algorithm to find one. Finally, we show that our model inherits many of the fairness properties of traditional equilibrium models. :::</li>
<li><strong>Truthful Multi-Parameter Auctions with Online Supply: an Impossible Combination</strong> [<a href="https://arxiv.org/pdf/1511.03699.pdf">arXiv pdf</a>| <a href="#TMOA">Show/Hide Abstract</a>].<br />
with Balasubramanian Sivan and Vasilis Syrgkanis. In Proc. <em>SODA 2018</em>. ::: {#absTMOA style=“display:none”} We study a basic auction design problem with online supply. There are two unit-demand bidders and two types of items. The first item type will arrive first for sure, and the second item type may or may not arrive. The auctioneer has to decide the allocation of an item immediately after each item arrives, but is allowed to compute payments after knowing how many items arrived. For this problem we show that there is no deterministic truthful and individually rational mechanism that, even with unbounded computational resources, gets any finite approximation factor to the optimal social welfare. :::</li>
<li><strong>Bubble Execution: Resource-aware Reliable Analytics at Cloud Scale.</strong> [<a href="pubs/Bubble_with_ack.pdf">pdf</a>| <a href="#BE">Show/Hide Abstract</a>].<br />
with Zhicheng Yin, Jin Sun, Ming Li, Jaliya Ekanayake, Haibo Lin, Marc Friedman, Jose A. Blakeley and Clemens A. Szyperski. In Proc. <em>VLDB 2018</em>. ::: {#absBE style=“display:none”} Enabling interactive data exploration at cloud scale requires minimizing end-to-end query execution latency, while guar- anteeing fault tolerance, and query execution under resource- constraints. Typically, such a query execution involves or- chestrating the execution of hundreds or thousands of re- lated tasks on cloud scale clusters. Without any resource constraints, all query tasks can be scheduled to execute si- multaneously (gang scheduling) while connected tasks stream data between them. When the data size referenced by a query increases, gang scheduling may be resource-wasteful or un-satisﬁable with a limited, per-query resource budget. This paper introduces Bubble Execution, a new query processing framework for interactive workloads at cloud scale, that balances cost-based query optimization, fault tolerance, optimal resource management, and execution orchestration. Bubble execution involves dividing a query execution graph into a collection of query sub-graphs (bubbles), and scheduling them within a per-query resource budget. The query operators (tasks) inside a bubble stream data between them while fault tolerance is handled by persisting temporary results at bubble boundaries. Our implementation enhances our JetScope service, for interactive workloads, deployed in production clusters at Microsoft. Experiments with TPC-H queries show that bubble execution can reduce resource usage significantly in the presence of failures while maintaining performance competitive with gang execution. :::</li>
</ul>
<p><strong>2017</strong></p>
<ul>
<li><p><strong>Stability of Service under Time-of-Use Pricing</strong> [ <a href="https://arxiv.org/pdf/1704.02364.pdf">arXiv pdf</a> | <a href="#SOS">Show/Hide Abstract</a>]<br />
with Shuchi Chawla, Alexander E. Holroyd, Anna Karlin, James Martin and Balasubramanian Sivan. In Proc. <em>STOC 2017</em> ::: {#absSOS style=“display:none”} We consider "time-of-use" pricing as a technique for matching supply and demand of temporal resources with the goal of maximizing social welfare. Relevant examples include energy, computing resources on a cloud computing platform, and charging stations for electric vehicles, among many others. A client/job in this setting has a window of time during which he needs service, and a particular value for obtaining it. We assume a stochastic model for demand, where each job materializes with some probability via an independent Bernoulli trial. Given a per-time-unit pricing of resources, any realized job will first try to get served by the cheapest available resource in its window and, failing that, will try to find service at the next cheapest available resource, and so on. Thus, the natural stochastic fluctuations in demand have the potential to lead to cascading overload events. Our main result shows that setting prices so as to optimally handle the expected demand works well: with high probability, when the actual demand is instantiated, the system is stable and the expected value of the jobs served is very close to that of the optimal offline algorithm. :::</p></li>
<li><p><strong>Truth and Regret in Online Scheduling</strong> [ <a href="https://arxiv.org/pdf/1703.00484.pdf">arXiv pdf</a> | <a href="#TROS">Show/Hide Abstract</a>]<br />
with Shuchi Chawla, Janardhan Kulkarni, Rad Niazadeh. In Proc. <em>EC 2017</em> ::: {#absTROS style=“display:none”} We consider a scheduling problem where a cloud service provider has multiple units of a resource available over time. Selfish clients submit jobs, each with an arrival time, deadline, length, and value. The service provider's goal is to implement a truthful online mechanism for scheduling jobs so as to maximize the social welfare of the schedule. Recent work shows that under a stochastic assumption on job arrivals, there is a single-parameter family of mechanisms that achieves near-optimal social welfare. We show that given any such family of near-optimal online mechanisms, there exists an online mechanism that in the worst case performs nearly as well as the best of the given mechanisms. Our mechanism is truthful whenever the mechanisms in the given family are truthful and prompt, and achieves optimal (within constant factors) regret. We model the problem of competing against a family of online scheduling mechanisms as one of learning from expert advice. A primary challenge is that any scheduling decisions we make affect not only the payoff at the current step, but also the resource availability and payoffs in future steps. Furthermore, switching from one algorithm (a.k.a. expert) to another in an online fashion is challenging both because it requires synchronization with the state of the latter algorithm as well as because it affects the incentive structure of the algorithms. We further show how to adapt our algorithm to a non-clairvoyant setting where job lengths are unknown until jobs are run to completion. Once again, in this setting, we obtain truthfulness along with asymptotically optimal regret (within poly-logarithmic factors). :::</p></li>
<li><p><strong>Convex Program Duality, Fisher Markets, and Nash Social Welfare</strong> [<a href="https://arxiv.org/pdf/1609.06654.pdf">arXiv pdf</a> | <a href="#CP">Show/Hide Abstract</a>].<br />
with Richard Cole, Vasilis Gkatzelis, Kamal Jain, Tung Mai, Vijay V. Vazirani and Sadra Yazdanbod. In Proc. <em>EC 2017</em> ::: {#absCP style=“display:none”} We study Fisher markets and the problem of maximizing the Nash social welfare (NSW), and show several closely related new results. In particular, we obtain:</p>
<ul>
<li>A new integer program for the NSW maximization problem whose fractional relaxation has a bounded integrality gap. In contrast, the natural integer program has an unbounded integrality gap.</li>
<li>An improved, and tight, factor 2 analysis of the algorithm of [7]; in turn showing that the integrality gap of the above relaxation is at most 2. The approximation factor shown by [7] was 2e^(1/e) ~ 2.89.</li>
<li>A lower bound of e^(1/e) ~ 1.44 on the integrality gap of this relaxation.</li>
<li>New convex programs for natural generalizations of linear Fisher markets and proofs that these markets admit rational equilibria.</li>
</ul>
<p>These results were obtained by establishing connections between previously known disparate results, and they help uncover their mathematical underpinnings. We show a formal connection between the convex programs of Eisenberg and Gale and that of Shmyrev, namely that their duals are equivalent up to a change of variables. Both programs capture equilibria of linear Fisher markets. By adding suitable constraints to Shmyrev's program, we obtain a convex program that captures equilibria of the spending-restricted market model defined by [7] in the context of the NSW maximization problem. Further, adding certain integral constraints to this program we get the integer program for the NSW mentioned above. The basic tool we use is convex programming duality. In the special case of convex programs with linear constraints (but convex objectives), we show a particularly simple way of obtaining dual programs, putting it almost at par with linear program duality. This simple way of finding duals has been used subsequently for many other applications. :::</p></li>
<li><p><strong>Optimal Multi-Unit Mechanisms with Private Demands</strong> [ <a href="https://arxiv.org/pdf/1704.05027.pdf">arXiv pdf</a> | <a href="#MUP">Show/Hide Abstract</a>]<br />
with Nima Haghpanah, Christos-Alexandros Psomas. In Proc. <em>EC 2017</em> ::: {#absMUP style=“display:none”} In the multi-unit pricing problem, multiple units of a single item are for sale. A buyer's valuation for n units of the item is v*min{n,d}, where the per unit valuation v and the capacity d are private information of the buyer. We consider this problem in the Bayesian setting, where the pair (v,d) is drawn jointly from a given probability distribution. In the unlimited supply setting, the optimal (revenue maximizing) mechanism is a pricing problem, i.e., it is a menu of lotteries. In this paper we show that under a natural regularity condition on the probability distributions, which we call decreasing marginal revenue, the optimal pricing is in fact deterministic. It is a price curve, offering i units of the item for a price of p_i, for every integer i. Further, we show that the revenue as a function of the prices p_i is a concave function, which implies that the optimum price curve can be found in polynomial time. This gives a rare example of a natural multi-parameter setting where we can show such a clean characterization of the optimal mechanism. We also give a more detailed characterization of the optimal prices for the case where there are only two possible demands. :::</p></li>
<li><p><strong>Online Auctions and Multi-scale Online Learning</strong> [ <a href="pubs/msol-jmlr.pdf">pdf</a> | <a href="#MSOL">Show/Hide Abstract</a>]<br />
with Sebastien Bubeck, Zhiyi Huang and Rad Niazadeh. In Proc. <em>EC 2017</em> ::: {#absMSOL style=“display:none”} We consider revenue maximization in online auctions and pricing. A seller sells an identical item in each period to a new buyer, or a new set of buyers. For the online posted pricing problem, we show regret bounds that scale with the best fixed price, rather than the range of the values. We also show regret bounds that are almost scale free, and match the offine sample complexity, when comparing to a benchmark that requires a lower bound on the market share. These results are obtained by generalizing the classical learning from experts and multi-armed bandit problems to their multi-scale versions. In this version, the reward of each action is in a different range, and the regret w.r.t. a given action scales with its own range, rather than the maximum range. :::</p></li>
<li><p><strong>The optimal mechanism for selling to a budget constrained buyer: the general case</strong> [ <a href="pubs/sib-ec.pdf">pdf</a> | <a href="#SIB">Show/Hide Abstract</a>]<br />
with Matt Weinberg. In Proc. <em>EC 2017</em> ::: {#absSIB style=“display:none”} We consider a revenue-maximizing seller with a single item facing a single buyer with a private budget. The (value, budget) pair is drawn from an arbitrary and possibly correlated distribution. We characterize the optimal mechanism in such cases, and quantify the amount of price discrimination that might be present. For example, there could be up to 3*2^(k-1)-1 distinct non-trivial menu options in the optimal mechanism for such a buyer with k distinct possible budgets (compared to k if the marginal distribution of values conditioned on each budget has decreasing marginal revenue [Che and Gale, 2000], or 2 if there is an arbitrary distribution and one possible budget [Chawla et al., 2011]). Our approach makes use of the duality framework of Cai et al. [2016], and duality techniques related to the "FedEx Problem" of Fiat et al. [2016]. In contrast to [Fiat et al., 2016] and other prior work, we characterize the optimal primal/dual without nailing down an explicit closed form. :::</p></li>
</ul>
<p><strong>2016</strong></p>
<ul>
<li><strong>Linear Contextual Bandits with Knapsacks</strong> [ <a href="pubs/NIPSLinContextualmain.pdf">NIPS pdf</a> | <a href="pubs/NIPSLinContextualSupplement.pdf">Supplement</a> | <a href="#LCB">Show/Hide Abstract</a>]<br />
with Shipra Agrawal. In Proc. <em>NIPS 2016</em> ::: {#absLCB style=“display:none”} We consider the linear contextual bandit problem with resource consumption, in addition to reward generation. In each round, the outcome of pulling an arm is a reward as well as a vector of resource consumptions. The expected values of these outcomes depend linearly on the context of that arm. The budget/capacity constraints require that the total consumption doesn’t exceed the budget for each resource. The objective is once again to maximize the total reward. This problem turns out to be a common generalization of classic linear contextual bandits (linContextual), bandits with knapsacks (BwK), and the online stochastic packing problem (OSPP). We present algorithms with near-optimal regret bounds for this problem. Our bounds compare favorably to results on the unstruc- tured version of the problem where the relation between the contexts and the outcomes could be arbitrary, but the algorithm only competes against a ﬁxed set of policies accessible through an optimization oracle. We combine techniques from the work on linContextual, BwK and OSPP in a nontrivial manner while also tackling new difﬁculties that are not present in any of these special cases. :::</li>
<li><strong>The Sample Complexity of Auctions with Side Information</strong> [<a href="http://arxiv.org/pdf/1511.02296.pdf">arXiv pdf</a>| <a href="#SCA">Show/Hide Abstract</a>]<br />
with Zhiyi Huang and Christos-Alexandros Psomas. In Proc. <em>STOC 2016</em> ::: {#absSCA style=“display:none”} Traditionally, the Bayesian optimal auction design problem has been considered either when the bidder values are i.i.d, or when each bidder is individually identifiable via her value distribution. The latter is a reasonable approach when the bidders can be classified into a few categories, but there are many instances where the classification of bidders is a continuum. For example, the classification of the bidders may be based on their annual income, their propensity to buy an item based on past behavior, or in the case of ad auctions, the click through rate of their ads. We introduce an alternate model that captures this aspect, where bidders are a priori identical, but can be distinguished based (only) on some side information the auctioneer obtains at the time of the auction. We extend the sample complexity approach of Dhangwatnotai et al. and Cole and Roughgarden to this model and obtain almost matching upper and lower bounds. As an aside, we obtain a revenue monotonicity lemma which may be of independent interest. We also show how to use Empirical Risk Minimization techniques to improve the sample complexity bound of Cole and Roughgarden for the non-identical but independent value distribution case. :::</li>
<li><strong>A Duality Based Unified Approach to Bayesian Mechanism Design</strong> [<a href="pubs/DualityAuctions.pdf">pdf</a>| <a href="#DBMD">Show/Hide Abstract</a>]<br />
with Yang Cai and Matt Weinberg. In Proc. <em>STOC 2016</em> ::: {#absDBMD style=“display:none”} We provide a unified view of many recent exciting developments in Bayesian mechanism design, including the black-box reductions of Cai et. al., simple mechanisms for additive buyers [Hart and Nisan, Li and Yao, Babaioff et al.], and posted-price mechanisms for unit-demand buyers [Chawla et al., Kleinberg and Weinberg]. Additionally, we show that viewing these three previously disjoint lines of work through the same lens allows us to improve upon each in several directions. First, our work provides a new and transparent duality framework for Bayesian mechanism design, which naturally accommodates multiple agents, and arbitrary objectives and feasibility constraints. Using this, we prove that either a posted-price mechanism, or the VCG mechanism with per-bidder entry fees is a constant-factor approximation to the optimal Bayesian IC mechanism whenever buyers are unit-demand or additive, unifying previous breakthroughs of Chawla et al. and Yao. In addition, we improve the approximation factor in Yao's work from 69 to 8. Finally, we show that this view also leads to improved structural characterizations in the Cai et. al. framework. :::</li>
<li><strong>An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives</strong> [<a href="http://arxiv.org/pdf/1506.03374.pdf">arXiv pdf</a>| <a href="#CBwK">Show/Hide Abstract</a>]<br />
with Shipra Agrawal and Lihong Li. In Proc. <em>COLT 2016</em> ::: {#absCBwK style=“display:none”} We consider a contextual version of multi-armed bandit problem with global knapsack constraints. In each round, the outcome of pulling an arm is a scalar reward and a resource consumption vector, both dependent on the context, and the global knapsack constraints require the total consumption for each resource to be below some pre-fixed budget. The learning agent competes with an arbitrary set of context-dependent policies. This problem was introduced by Badanidiyuru et al., who gave a computationally inefficient algorithm with near optimal regret bounds for it. We give a computationally efficient algorithm for this problem with slightly better regret bounds, by generalizing the approach of Agarwal et al. for the non-constrained version of the problem. The computational time of our algorithm scales logarithmically in the size of the policy space. This answers the main open question of Badanidiyuru et al. We also extend our results to a variant where there are no knapsack constraints but the objective is an arbitrary Lipschitz concave function of the sum of outcome vectors. :::</li>
<li><strong>ProjecToR: Agile Reconfigurable Datacenter Interconnect</strong> [ <a href="pubs/projector-sigcomm-cr">pdf</a> | <a href="#CBwK">Show/Hide Abstract</a>]<br />
with M. Ghobadi, R. Mahajan, A. Phanishayee, H. Rastegarfar, P. Blanche, M. Glick, D. Kilper, J. Kulkarni and G. Ranade. In Proc. <em>SIGCOMM 2016</em> ::: {#absPTOR style=“display:none”} We explore a radically different approach for building data center interconnects--using free-space optics between racks, it enables all rack-pairs to communicate via direct links and can reconfigure such links within 12us. Our approach uses a digital micromirror device (DMD) and mirror assembly combination as a transmitter and a photodetector on top of the rack as a receiver. Transmitters and receivers in our interconnect can be dynamically linked in billions of ways. We develop topology construction and routing methods to exploit this flexibility, including a new flow scheduling algorithm that is a constant factor approximation to the offline optimal solution. We build a small prototype that points to the feasibility of our approach. Simulations and analysis show that, for realistic data center workloads, it can improve mean flow completion time by 30-95%, while reducing cost by 25-40%. :::</li>
<li><strong>Multi-Score Position Auctions</strong> [<a href="pubs/DSA.pdf">pdf</a>| <a href="#DSA">Show/Hide Abstract</a>]<br />
with Denis X. Charles and Balasubramanian Sivan. In Proc. <em>WSDM 2016.</em> ::: {#absDSA style=“display:none”} In this paper we propose a general family of position auctions used in paid search, which we call multi-score position auctions. These auctions contain the GSP auction and the GSP auction with squashing as special cases. We show experimentally that these auctions contain special cases that perform better than the GSP auction with squashing, in terms of revenue, and the number of clicks on ads. In particular, we study in detail the special case that squashes the first slot alone and show that this beats pure squashing (which squashes all slots uniformly). We study the equilibria that arise in this special case to examine both the first order and the second order effect of moving from the squashing-all-slots auction to the squash-only-the-top-slot auction. For studying the second order effect, we simulate auctions using the value-relevance correlated distribution suggested in Lahaie and Pennock [2007]. Since this distribution is derived from a study of value and relevance distributions in Yahoo! we believe the insights derived from this simulation to be valuable. For measuring the first order effect, in addition to the said simulation, we also conduct experiments using auction data from Bing over several weeks that includes a random sample of all auctions. :::</li>
<li><strong>Simple Pricing Schemes For Consumers With Evolving Values</strong> [<a href="pubs/ppp.pdf">pdf</a>| <a href="#PPP">Show/Hide Abstract</a>]<br />
with Shuchi Chawla, Anna Karlin and Balasubramanian Sivan. In Proc. <em>SODA 2016.</em> ::: {#absPPP style=“display:none”} We consider a pricing problem where a buyer is interested in purchasing/using a good, such as an app or music or software, repeatedly over time. The consumer discovers his value for the good only as he uses it, and the value evolves with each use. Optimizing for the seller's revenue in such dynamic settings is a complex problem and requires assumptions about how the buyer behaves before learning his future value(s), and in particular, how he reacts to risk. We explore the performance of a class of pricing mechanisms that are extremely simple for both the buyer and the seller to use: the buyer reacts to prices myopically without worrying about how his value evolves in the future; the seller needs to optimize for revenue over a space of only two parameters, and can do so without knowing the buyer's risk profile or fine details of the value evolution process. We present simple-versus-optimal type results, namely that under certain assumptions, simple pricing mechanisms of the above form are approximately optimal <em>regardless of the buyer's risk profile</em> . Our results assume that the buyer's value per usage evolves as a martingale. For our main result, we consider pricing mechanisms in which the seller offers the product for free for a certain number of uses, and then charges an appropriate fixed price per usage. We assume that the buyer responds by buying the product for as long as his value exceeds the fixed price. Importantly, the buyer does not need to know anything about how his future value will evolve, only how much he wants to use the product <em>right now</em> . Regardless of the buyers' initial value, our pricing captures as revenue a constant fraction of the total value that the buyers accumulate in expectation over time. :::</li>
</ul>
<p><strong>2015</strong></p>
<ul>
<li><strong>Speed Scaling in the Non-clairvoyant Model</strong> [<a href="pubs/spaa035-azarA.pdf">pdf</a>| <a href="#NCS">Show/Hide Abstract</a>]<br />
with Yossi Azar, Zhiyi Huang and Debmalya Panigrahi. In Proc. <em>SPAA 2015.</em> Winner of the <strong>Best Paper award.</strong> ::: {#absNCS style=“display:none”} In recent years, there has been a growing interest in speed scaling algorithms, where a set of jobs need to be scheduled on a machine with variable speed so as to optimize the flow-times of the jobs and the energy consumed by the machine. A series of results have culminated in constant-competitive algorithms for this problem in the clairvoyant model, i.e., when job parameters are revealed on releasing a job (Bansal, Pruhs, and Stein, SODA 2007; Bansal, Chan, and Pruhs, SODA 2009). Our main contribution in this paper is the first constant-competitive speed scaling algorithm in the non- clairvoyant model, which is typically used in the scheduling literature to model practical settings where job volume is revealed only after the job has been completely processed. Unlike in the clairvoyant model, the speed scaling problem in the non-clairvoyant model is non-trivial even for a single job. Our non-clairvoyant algorithm is defined by using the existing clairvoyant algorithm in a novel inductive way, which then leads to an inductive analytical tool that may be of independent interest for other online optimization problems. We also give additional algorithmic results and lower bounds for speed scaling on multiple identical parallel machines. :::</li>
<li><strong>Simple Auctions with Simple Strategies</strong> [<a href="pubs/Draft-OneShot.pdf">pdf</a>| <a href="#CP">Show/Hide Abstract</a>]<br />
with Jamie Morgenstern, Vasilis Syrgkanis and Matt Weinberg. In Proc. <em>EC 2015</em> ::: {#absDA style=“display:none”} We introduce single-bid auctions as a new format for combinatorial auctions. In single-bid auctions, each bidder submits a single real-valued bid for the right to buy items at a fixed price. Contrary to other simple auction formats, such as simultaneous or sequential single-item auctions, bidders can implement no-regret learning strategies for single-bid auctions in polynomial time. Price of anarchy bounds for correlated equilibria concepts in single-bid auctions therefore have more bite than their counterparts for auctions and equilibria for which learning is not known to be computationally tractable (or worse, known to be computationally intractable). Towards this end, we show that for any subadditive valuations the social welfare at equilibrium is an O(log m) approximation to the optimal social welfare, where m is the number of items. We also provide tighter approximation results for several subclasses. Our welfare guarantees hold for Nash equilibria and no-regret learning outcomes in both Bayesian and complete information settings via the smooth-mechanism framework. Of independent interest, our techniques show that in a combinatorial auction setting, efficiency guarantees of a mechanism via smoothness for a very restricted class ofcardinality it valuations extend, with a small degradation, to subadditive valuations, the largest complement-free class of valuations. :::</li>
<li><strong>Revenue Maximization and Ex-Post Budget Constraints</strong> [<a href="pubs/Revenue%20Max%20Budget%20Constraints.pdf">pdf</a>| <a href="#RMBC">Show/Hide Abstract</a>]<br />
with Constantinos Daskalakis and Matt Weinberg. In Proc. <em>EC 2015</em> ::: {#absRMBC style=“display:none”} We consider the problem of a revenue-maximizing seller with m items for sale to n additive bidders with hard budget constraints, assuming that the seller has some prior distribution over bidder values and budgets. The prior may be correlated across items and budgets of the same bidder, but is assumed independent across bidders. We target mechanisms that are Bayesian Incentive Compatible, but that are ex-post Individually Rational and ex-post budget respecting. Virtually no such mechanisms are known that satisfy all these conditions and guarantee any revenue approximation, even with just a single item. We provide a computationally efﬁcient mechanism that is a 3-approximation with respect to all BIC, ex-post IR, and ex-post budget respecting mechanisms. Note that the problem is NP-hard to approximate better than a factor of 16/15, even in the case where the prior is a point mass [Chakrabarty and Goel 2010]. We further characterize the optimal mechanism in this setting, showing that it can be interpreted as a distribution over virtual welfare maximizers. We prove our results by making use of a black-box reduction from mechanism to algorithm design developed by [Cai et al. 2013]. Our main technical contribution is a computationally efﬁcient 3-approximation algorithm for the algorithmic problem that results by an application of their framework to this problem. The algorithmic problem has a mixed-sign objective and is NP-hard to optimize exactly, so it is surprising that a computationally efﬁcient approximation is possible at all. In the case of a single item (m = 1), the algorithmic problem can be solved exactly via exhaustive search, leading to a computationally efﬁcient exact algorithm and a stronger characterization of the optimal mechanism as a distribution over virtual value maximizers. :::</li>
<li><strong>Fast Algorithms for Online Stochastic Convex Programming</strong> [<a href="pubs/ConvexSecretary.pdf">pdf</a>|<a href="#FAOSCP">Show/Hide Abstract</a>]<br />
with Shipra Agrawal. In Proc. <em>SODA 2015</em> ::: {#absFAOSCP style=“display:none”} We introduce the online stochastic Convex Programming (CP) problem, a very general version of stochas- tic online problems which allows arbitrary concave objectives and convex feasibility constraints. Many well- studied problems like online stochastic packing and covering, online stochastic matching with concave returns, etc. form a special case of online stochastic CP. We present fast algorithms for these problems, which achieve near-optimal regret guarantees for both the i.i.d. and the random permutation models of stochastic inputs. When applied to the special case online packing, our ideas yield a simpler and faster primal-dual algorithm for this well studied problem, which achieves the optimal competitive ratio. Our techniques make explicit the connection of primal-dual paradigm and online learning to online stochastic CP. :::</li>
<li><strong>Perfect Bayesian Equilibria in Repeated Sales</strong> [<a href="http://arxiv.org/abs/1409.3062.pdf">Arxiv pdf</a> | <a href="#PBE">Show/Hide Abstract</a>]<br />
with Yuval Peres and Balasubramanian Sivan. In Proc. <em>SODA 2015</em> ::: {#absPBE style=“display:none”} A special case of Myerson's classic result describes the revenue-optimal equilibrium when a seller offers a single item to a buyer. We study a repeated sales extension of this model: a seller offers to sell a single fresh copy of an item to the same buyer every day via a posted price. The buyer's value for the item is unknown to the seller but is drawn initially from a publicly known distribution F and remains the same throughout. We study this setting where the seller is unable to commit to future prices and find several surprises. First, if the horizon is fixed, previous work showed that an equilibrium exists, and all equilibria yield tiny or constant revenue. This is a far cry from the linearly growing benchmark of getting Myerson optimal revenue each day. Our first result shows that this is because the buyer strategies in these equilibria are necessarily unnatural. We restrict to a natural class of buyer strategies called threshold strategies, and show that threshold equilibria rarely exist. Second, if the seller can commit not to raise prices upon purchase, while still retaining the possibility of lowering prices in future, we show that threshold equilibria always exist and for most distributions there is a unique threshold equilibrium. As an example, if F is uniform in [0,1], the seller can extract revenue of order $\sqrt{n}$ in $n$ rounds as opposed to the constant revenue obtainable when he is unable to make commitments. Finally, we consider the infinite horizon game, where both the seller and the buyer discount the future utility by a factor of $1-\delta \in [0,1)$. When the value distribution is uniform in [0,1], there exists a threshold equilibrium with expected revenue at least $\frac{4}{3+2\sqrt{2}} \sim 69$\% of the Myerson optimal revenue benchmark. This is in sharp contrast to the constant revenue obtained in the limit of the $n$-stage games as $n$ approaches infinity. :::</li>
<li><strong>Budget Constraints in Prediction Markets</strong> [<a href="pubs/ddhpuai15.pdf">pdf</a>| <a href="#BCPM">Show/Hide Abstract</a>]<br />
with Miro Dudik, Zhiyi Huang and Dave Pennock. In Proc. <em>UAI 2015</em> ::: {#absBCPM style=“display:none”} An automated market maker is a natural and common mechanism to subsidize information acquisition, revelation, and aggregation in a prediction market. The sought-after prediction aggregate is the equilibrium price. However, traders with budget constraints are restricted in their ability to impact the market price on their own. We give a detailed characterization of optimal trades in the presence of budget constraints in a prediction market with a cost-function-based automated market maker. As a concrete application of our characterization, we give sufficient conditions for a property we call budget additivity: two traders with budgets B and B' and the same beliefs would have a combined impact equal to a single trader with budget B +B'. That way, even if a single trader cannot move the market much, a crowd of like-minded traders can have the same desired effect. We show that a generalization of the heavily-used logarithmic market scoring rule is budget additive for affinely independent payoffs, but the quadratic market scoring rule is not. Our results may be used both descriptively, to understand if a particular market maker is affected by budget constraints or not, and prescriptively, as a recipe to construct markets. :::</li>
</ul>
<p><strong>2014</strong></p>
<ul>
<li><strong>Envy freedom and prior-free mechanism design</strong> [<a href="http://www.sciencedirect.com/science/article/pii/S0022053114001094">JET</a> | <a href="http://arxiv.org/pdf/1212.3741.pdf">Arxiv pdf</a> | <a href="#EFPF">Show/Hide Abstract</a>]<br />
with Jason D. Hartline and Qiqi Yan. In <em>Journal of Economic Theory, 2014</em> ::: {#absEFPF style=“display:none”} We consider the provision of an abstract service to single-dimensional agents. Our model includes position auctions, single-minded combinatorial auctions, and constrained matching markets. When the agents' values are drawn independently from a distribution, the Bayesian optimal mechanism is given by Myerson as a virtual-surplus optimizer. We develop a framework for prior-free mechanism design and analysis. A good mechanism in our framework approximates the optimal mechanism for the distribution if there is a distribution; moreover, when there is no distribution this mechanism still provably performs well.<br />
We define and characterize optimal envy-free outcomes in symmetric single-dimensional environments. Our characterization mirrors Myerson's theory. Furthermore, unlike in mechanism design where there is no point-wise optimal mechanism, there is always a point-wise optimal envy-free outcome.<br />
Envy-free outcomes and incentive-compatible mechanisms are similar in structure and performance. We therefore use the optimal envy-free revenue as a benchmark for measuring the performance of a prior-free mechanism. A good mechanism is one that approximates the envy-free benchmark on any profile of agent values. We show that good mechanisms exist, and in particular, a natural generalization of the random sampling auction of Goldberg et al. is a constant approximation. :::</li>
<li><strong>Bandits with concave rewards and convex knapsacks</strong> [<a href="http://arxiv.org/pdf/1402.5758v1.pdf">Arxiv pdf</a> | <a href="#BwCR">Show/Hide Abstract</a>]<br />
with Shipra Agrawal. In Proc. <em>ACM EC 2014</em> ::: {#absBwCR style=“display:none”} In this paper, we consider a very general model for exploration-exploitation tradeoff which allows arbitrary concave rewards and convex constraints on the decisions across time, in addition to the customary limitation on the time horizon. This model subsumes the classic multi-armed bandit (MAB) model, and the Bandits with Knapsacks (BwK) model of Badanidiyuru et al.[2013]. We also consider an extension of this model to allow linear contexts, similar to the linear contextual extension of the MAB model. We demonstrate that a natural and simple extension of the UCB family of algorithms for MAB provides a polynomial time algorithm that has near-optimal regret guarantees for this substantially more general model, and matches the bounds provided by Badanidiyuru et al.[2013] for the special case of BwK, which is quite surprising. We also provide computationally more efficient algorithms by establishing interesting connections between this problem and other well studied problems/algorithms such as the Blackwell approachability problem, online convex optimization, and the Frank-Wolfe technique for convex optimization. We give examples of several concrete applications, where this more general model of bandits allows for richer and/or more efficient formulations of the problem. :::</li>
<li><strong>Removing Arbitrage from Wagering Mechanisms</strong> [<a href="pubs/Wagering-full.pdf">pdf</a> | <a href="#NAWM">Show/Hide Abstract</a>]<br />
with Yiling Chen, David Pennock, Jennifer Wortman Vaughan. In Proc. <em>ACM EC 2014</em> ::: {#absNAWM style=“display:none”} We observe that Lambert et al.'s [2008] family of weighted score wagering mechanisms admit arbitrage: participants can extract a guaranteed positive payoﬀ by betting on any prediction within a certain range. In essence, participants leave free money on the table when they "agree to disagree", and as a result, rewards don’t necessarily go to the most informed and accurate participants. This observation suggests that when participants have immutable beliefs, it may be possible to design alternative mechanisms in which the center can make a proﬁt by removing this arbitrage opportunity without sacriﬁcing incentive properties such as individual rationality, incentive compatibility, and sybilproofness. We introduce a new family of wagering mechanisms called no-arbitrage wagering mechanisms that retain many of the positive properties of weighted score wagering mechanisms, but with the arbitrage opportunity removed. We show several structural results about the class of mechanisms that satisfy no-arbitrage in conjunction with other properties, and provide examples of no-arbitrage wagering mechanisms with interesting properties. :::</li>
<li><strong>Primal dual gives optimal energy efficient online algorithms</strong> [<a href="pubs/DH14-SODA.pdf">pdf</a> | <a href="#DH14">Show/Hide Abstract</a>]<br />
with Zhiyi Huang. In Proc. <em>SODA 2014</em> ::: {#absDH-14 style=“display:none”} We consider the problem of online scheduling of jobs on unrelated machines with dynamic speed scaling to minimize the sum of energy and weighted flow time. We give an algorithm with an almost optimal competitive ratio for arbitrary power functions. (No earlier results handled arbitrary power functions for unrelated machines.) For power functions of the form $f(s) = s^\alpha$ for some constant $\alpha&gt;1$, we get a competitive ratio of $O(\tfrac {\alpha} {\log \alpha}) $, improving upon a previous competitive ratio of $O(\alpha^2)$ by Anand et al., along with a matching lower bound of $\Omega(\tfrac {\alpha} {\log \alpha})$. Further, in the resource augmentation model, with a 1+ $\epsilon$ speed up, we give a $2( \tfrac 1 \epsilon+1)$ competitive algorithm, with essentially the same techniques, improving the bound of $1 + O(\frac{1}{\epsilon^2})$ by Gupta et al. and matching the bound of Anand et al. for the special case of fixed speed unrelated machines. Unlike the previous results most of which used an amortized local competitiveness argument or dual fitting methods, we use a primal-dual method, which is useful not only to analyze the algorithms but also to design the algorithm itself. :::</li>
</ul>
<p><strong>2013</strong></p>
<ul>
<li><p><strong>Whole-page Optimization and Submodular Welfare Maximization with Online Bidders</strong> [<a href="pubs/ec-free-disposal.pdf">pdf</a> | <a href="#CP">Show/Hide Abstract</a>]<br />
with Zhiyi Huang, Nitish Korula, Vahab Mirrokni and Qiqi Yan. In Proc. <em>ACM EC 2013</em> ::: {#absFD style=“display:none”} In the context of online ad serving, display ads may appear on different types of web-pages, where each page includes several ad slots and therefore multiple ads can be shown on each page. The set of ads that can be assigned to ad slots of the same page needs to satisfy various pre-specified constraints including exclusion constraints, diversity constraints, and the like. Upon arrival of a user, the ad serving system needs to allocate a set of ads to the current web-page respecting these per-page allocation constraints. Previous slot-based settings ignore the important concept of a page, and may lead to highly suboptimal results in general. In this paper, motivated by these applications in display advertising and inspired by the submodular welfare maximization problem with online bidders, we study a general class of page-based ad allocation problems, present the first (tight) constant-factor approximation algorithms for these problems, and confirm the performance of our algorithms experimentally on real-world data sets. A key technical ingredient of our results is a novel primal-dual analysis for handling free-disposal, which updates dual variables using a "level function" instead of a single level, and unifies with previous analyses of related problems. This new analysis method allows us to handle arbitrarily complicated allocation constraints for each page. Our main result is an algorithm that achieves a 1 - 1/e - o(1) competitive ratio. Moreover, our experiments on real-world data sets show signi?cant improvements of our page-based algorithms compared to the slot-based algorithms. Finally, we observe that our problem is closely related to the submodular welfare maximization (SWM) problem. In particular, we introduce a variant of the SWM problem with online bidders, and show how to solve this problem using our algorithm for whole page optimization. :::</p></li>
<li><p><strong>Budget Smoothing for Internet Ad Auctions: A Game Theoretic Approach</strong> [<a href="pubs/ec44-charles.pdf">pdf</a> | <a href="#CP">Show/Hide Abstract</a>]<br />
with Deeparnab Chakrabarty, Denis Charles, Max Chickering and Lei Wang. In Proc. <em>ACM EC 2013</em> ::: {#absBS style=“display:none”} In Internet ad auctions, search engines often throttle budget constrained advertisers so as to spread their spends across the specified time period. Such policies are known as budget smoothing policies. In this paper, we perform a principled, game-theoretic study of what the outcome of an ideal budget smoothing algorithm should be. In particular, we propose the notion of regret-free budget smoothing policies whose outcomes throttle each advertiser optimally, given the participation of the other advertisers. We show that regret-free budget smoothing policies always exist, and in the case of single slot auctions we can give a polynomial time smoothing algorithm. Inspired by the existence proof, we design a heuristic for budget smoothing which performs considerably better than existing benchmark heuristics. :::</p></li>
<li><p><strong>Prior-free Auctions for Budgeted Agents</strong> [<a href="pubs/ec14-devanur.pdf">pdf</a> | <a href="#CP">Show/Hide Abstract</a>]<br />
with Bach Q. Ha and Jason D. Hartline. In Proc. <em>ACM EC 2013</em> ::: {#absPFABA style=“display:none”} We consider prior-free auctions for revenue and welfare maximization when agents have a common budget. The abstract environments we consider are ones where there is a downward-closed and symmetric feasibility constraint on the probabilities of service of the agents. These environments include position auctions where slots with decreasing click-through rates are auctioned to advertisers. We generalize and characterize the envy-free benchmark from Hartline and Yan [2011] to settings with budgets and characterize the optimal envy-free outcomes for both welfare and revenue. We give prior-free mechanisms that approximate these benchmarks. A building block in our mechanism is a clinching auction for position auction environments. This auction is a generalization of the multi-unit clinching auction of Dobzinski et al. [2008] and a special case of the polyhedral clinching auction of Goel et al. [2012]. For welfare maximization, we show that this clinching auction is a good approximation to the envy-free optimal welfare for position auction environments. For profit maximization, we generalize the random sampling profit extraction auction from Fiat et al. [2002] for digital goods to give a 10.0-approximation to the envy-free optimal revenue in symmetric, downward- closed environments. Even without budgets this revenue maximization question is of interest and we obtain an improved approximation bound of 7.5 (from 30.4 by Ha and Hartline [2012]). :::</p></li>
<li><p><strong>Tatonnement Beyond Gross Substitutes? Gradient Descent to the Rescue</strong> [<a href="pubs/tatonnement-gradient-descent.pdf">pdf</a> | <a href="#CP">Show/Hide Abstract</a>]<br />
with Yun Kuen Cheung and Richard Cole. In Proc. <em>STOC 2013</em>. To appear in <em>GEB</em> . ::: {#absTAT style=“display:none”} Tatonnement is a simple and natural rule for updating prices in Exchange (Arrow-Debreu) markets. In this paper we de- ?ne a class of markets for which tatonnement is equivalent to gradient descent. This is the class of markets for which there is a convex potential function whose gradient is always equal to the negative of the excess demand and we call it Convex Potential Function (CPF) markets. We show the following results.</p>
<ul>
<li>CPF markets contain the class of Eisenberg Gale (EG) markets, de?ned previously by Jain and Vazirani.</li>
<li>The subclass of CPF markets for which the demand is a differentiable function contains exactly those mar- kets whose demand function has a symmetric negative semi-definite Jacobian.</li>
<li>We de?ne a family of continuous versions of taton- nement based on gradient descent using a Bregman divergence. As we show, all processes in this family converge to an equilibrium for any CPF market. This is analogous to the classic result for markets satisfying the Weak Gross Substitutes property.</li>
<li>A discrete version of tatonnement converges toward the equilibrium for the following markets of comple- mentary goods; its convergence rate for these settings is analyzed using a common potential function.
<ul>
<li>Fisher markets in which all buyers have Leontief utilities. The tatonnement process reduces the distance to the equilibrium, as measured by the potential function, to an epsilon fraction of its initial value in O(1/epsilon) rounds of price updates.</li>
<li>Fisher markets in which all buyers have comple- mentary CES utilities. Here, the distance to thequilibrium is reduced to epsilon fraction of its initial value in O(log(1/epsilon)) rounds of price updates.</li>
</ul>
This shows that tatonnement converges for the entire range of Fisher markets when buyers have complementary CES utilities, in contrast to prior work, which could analyze only the substitutes range, together with a small portion of the complementary range. :::</li>
</ul></li>
<li><p><strong>Randomized Primal-Dual Analysis of RANKING for Online Bipartite Matching</strong> [<a href="pubs/RPDFinal.pdf">pdf</a> | <a href="#PR">Show/Hide Abstract</a>]<br />
with Kamal Jain and Bobby Kleinberg. In Proc. <em>SODA 2013</em> ::: {#absKVV style=“display:none”} We give a simple proof that the RANKING algorithm of Karp, Vazirani and Vazirani is 1-1/e competitive for the online bipartite matching problem. The proof is via a randomized primal-dual argument. Primal-dual algorithms have been successfully used for many online algorithm problems, but the dual constraints are always satisfied deterministically. This is the first instance of a non-trivial randomized primal-dual algorithm in which the dual constraints only hold in expectation. The approach also generalizes easily to the vertex-weighted version considered by Agarwal et al. Further we show that the proof is very similar to the deterministic primal-dual argument for the online budgeted allocation problem with small bids (also called the AdWords problem) of Mehta et al. :::</p></li>
</ul>
<p><strong>2012</strong></p>
<ul>
<li><p><strong>Asymptotically Optimal Algorithm for Stochastic Adwords</strong> [<a href="pubs/AOASA.pdf">pdf</a> | <a href="#PR">Show/Hide Abstract</a>]<br />
with Balasubramanian Sivan and Yossi Azar. In Proc. <em>EC 2012</em> ::: {#absAOA style=“display:none”} In this paper we consider the adwords problem in the <em>unknown distribution</em> model. We consider the case where the budget to bid ratio $k$ is at least 2, and give improved competitive ratios. Earlier results had competitive ratios better than $1-1/e$ only for ``large enough'' $k$, while our competitive ratio increases continuously with $k$. For $k=2$ the competitive ratio we get is $0.729$ and it is $0.9$ for $k=16$. We also improve the asymptotic competitive ratio for large $k$ from $1 - O(\sqrt{\log n /k})$ to $1 - O(\sqrt{1 /k})$, thus removing any dependence on $n$, the number of advertisers. This ratio is optimal, even with known distributions. That is, even if an algorithm is tailored to the distribution, it cannot get a competitive ratio of $1 - o(\sqrt{1 /k})$, whereas our algorithm does not depend on the distribution. The algorithm is rather simple, it computes a <em>score</em> for every advertiser based on his original budget, the remaining budget and the remaining number of steps in the algorithm and assigns a query to the advertiser with the highest bid plus his score. The analysis is based on a ``hybrid argument'' that considers algorithms that are part actual, part hypothetical, to prove that our (actual) algorithm is better than a completely hypothetical algorithm whose performance is easy to analyze. :::</p></li>
<li><p><strong>Online Matching with Concave Returns</strong> [<a href="pubs/OMwCR.pdf">pdf</a> | <a href="#PR">Show/Hide Abstract</a>]<br />
with Kamal Jain. In Proc. <em>STOC 2012</em> ::: {#absCM style=“display:none”} We consider a <em>significant</em> generalization of the Adwords problem by <em>allowing arbitrary concave returns, and we characterize the optimal competitive ratio achievable</em>. The problem considers a sequence of items arriving online that have to be allocated to agents, with different agents bidding different amounts. The objective function is the sum, over each agent i, of a monotonically non-decreasing concave function $M_i : R_+ \rightarrow R_+$ of the total amount allocated to i. All variants of online matching problems (including the Adwords problem) studied in the literature consider the special case of budgeted linear functions, that is, functions of the form $M_i( u_i) = \min \{u_i,B_i\}$ for some constant $B_i$. The distinguishing feature of this paper is in allowing arbitrary concave returns. The main result of this paper is that for each concave function $M$, there exists a constant $F(M) \leq 1$ such that</p>
<ul>
<li>there exists an algorithm with competitive ratio of \\$\min_i\{ F(M_i) \}$, independent of the sequence of items.</li>
<li>No algorithm has a competitive ratio larger than $F(M)$ over all instances with $M_i= M$ for all $i$.</li>
</ul>
<p>Our algorithm is based on the primal-dual paradigm and makes use of convex programming duality. The upper bounds are obtained by formulating the task of finding the right counterexample as an optimization problem. This path takes us through the calculus of variations which deals with optimizing over continuous functions. The algorithm and the upper bound are related to each other via a set of differential equations, which points to a certain kind of duality between them. :::</p></li>
</ul>
<p><strong>2011</strong></p>
<ul>
<li><strong>Real-Time Bidding Algorithms for Performance-Based Display Ad Allocation</strong> [<a href="pubs/rtb-perf.pdf">pdf</a>| <a href="#PR">Show/Hide Abstract</a>]<br />
with Ye Chen, Pavel Berkhin and Bo Anderson . In Proc. <em>KDD 2011</em> ::: {#absRTB style=“display:none”} We describe a real-time bidding algorithm for performance-based display ad allocation. A central issue in performance display advertising is matching campaigns to ad impressions, which can be formulated as a constrained optimization problem that maximizes revenue subject to constraints such as budget limits and inventory availability. The current practice is to solve the optimization problem offline at a tractable level of impression granularity (e.g., the placement level), and to serve ads online based on the precomputed static delivery scheme. Although this offline approach takes a global view to achieve optimality, it fails to scale to ad delivery decision making at an individual impression level. Therefore, we propose a real-time bidding algorithm that enables fine-grained impression valuation (e.g., targeting users with real-time conversion data), and adjusts value-based bid according to real-time constraint snapshot (e.g., budget consumption level). Theoretically, we show that under a linear programming (LP) primal-dual formulation, the simple real-time bidding algorithm is indeed an online solver to the original primal problem by taking the optimal solution to the dual problem as input. In other words, the online algorithm guarantees the offline optimality given the same level of knowledge an offline optimization would have. Empirically, we develop and experiment with two real-time bid adjustment approaches to adapting to the non-stationary nature of the marketplace: one adjusts bids against real-time constraint satisfaction level using control-theoretic methods, and the other adjusts bids also based on the historical bidding landscape statistically modeled. Finally, we show experimental results with real-world ad serving data. :::</li>
<li><strong>Online Algorithms with Stochastic Input</strong> [<a href="pubs/AdwordsStochastic.pdf">pdf</a> ]<br />
In <em>ACM SIGEcom Exchanges, Vol. 10, No. 2, June 2011, Pages 40- 49.</em></li>
<li><strong>Near Optimal Online Algorithms and Fast Approximation Algorithms for Resource Allocation Problems</strong> [<a href="pubs/GWLfull.pdf">pdf</a> | <a href="#PR">Show/Hide Abstract</a>]<br />
with Kamal Jain, Balasubramanian Sivan and Christopher A. Wilkens In Proc. <em>ACM EC 2011</em> ::: {#absGWL style=“display:none”} We present algorithms for a class of resource allocation problems both in the online setting with stochastic input and in the ofﬂine setting. This class of problems contains many interesting special cases such as the Adwords problem. In the online setting we introduce a new distributional model called the adversarial stochastic input model, which is a generalization of the i.i.d model with unknown distributions, where the distributions can change over time. In this model we give a 1 - O(epsilon) approximation algorithm for the resource allocation problem, with almost the weakest possible assumption: the ratio of the maximum amount of resource consumed by any single request to the total capacity of the resource, and the ratio of the proﬁt contributed by any single request to the optimal proﬁt is at most O (epsilon^2 /log(n/epsilon) where n is the number of resources available. There are instances where this ratio is epsilon^2/log n such that no randomized algorithm can have a competitive ratio of 1 - o(epsilon^2) even in the i.i.d model. The upper bound on ratio that we require improves on the previous upper-bound for the i.i.d case by a factor of n. Our proof technique also gives a very simple proof that the greedy algorithm has a competitive ratio of 1 - 1/e for the Adwords problem in the i.i.d model with unknown distributions, and more generally in the adversarial stochastic input model, when there is no bound on the bid to budget ratio. All the previous proofs assume that either bids are very small compared to budgets or something very similar to this. In the ofﬂine setting we give a fast algorithm to solve very large LPs with both packing and covering constraints. We give algorithms to approximately solve (within a factor of 1 + epsilon) the mixed packing-covering problem with O(gamma m log(n/delta)/epsilon^2 ) oracle calls where the constraint matrix of this LP has dimension n x m, the success probability of the algorithm is 1 - delta, and gamma is a parameter which is very similar to the ratio described for the online setting. We discuss several applications, and how our algorithms improve existing results in some of these applications. :::</li>
<li><strong>Distributed Algorithms via Gradient Descent for Fisher Markets</strong> [<a href="pubs/propresponse.pdf">pdf</a> | <a href="#PR">Show/Hide Abstract</a>]<br />
with Benjamin Birnbaum and Lin Xiao. In Proc. <em>ACM EC 2011</em> ::: {#absPR style=“display:none”} Designing distributed algorithms that converge quickly to an equi- librium is one of the foremost research goals in algorithmic game theory, and convex programs have played a crucial role in the de- sign of algorithms for Fisher markets. In this paper we shed new light on both aspects for Fisher markets with linear and spending constraint utilities. We show fast convergence of the Proportional Response dynamics recently introduced by Wu and Zhang [WZ07]. The convergence is obtained from a new perspective: we show that the Proportional Response dynamics is equivalent to a gradient de- scent algorithm (with respect to a Bregman divergence instead of euclidean distance) on a convex program that captures the equilib- ria for linear utilities. We further show that the convex program program easily extends to the case of spending constraint utilities, thus resolving an open question raised by [Vaz10]. This also gives a way to extend the Proportional Response dynamics to spending constraint utilties. We also prove a technical result that is interest- ing in its own right: that the gradient descent algorithm based on a Bregman divergence converges with rate O(1/t) under a condition that is weaker than having Lipschitz continuous gradient (which is the usual assumption in the optimization literature for obtaining the same rate). :::</li>
</ul>
<p><strong>2010</strong></p>
<ul>
<li><strong>Fast Algorithms for Finding Matchings in Lopsided Bipartite Graphs with Applications to Display Ads</strong> [<a href="pubs/waterlevel.pdf">pdf</a> | <a href="#PR">Show/Hide Abstract</a>]<br />
with Denis Charles, Max Chickering, Kamal Jain and Manan Sanghi. In Proc. <em>ACM EC 2010</em> ::: {#absWL style=“display:none”} We derive efficient algorithms for both detecting and representing matchings in lopsided bipartite graphs; such graphs have so many nodes on one side that it is infeasible to represent them in memory or to identify matchings using standard approaches. Detecting and representing matchings in lopsided bipartite graphs is important for allocating and delivering guaranteed-placement display ads, where the corresponding bipartite graph of interest has nodes representing advertisers on one side and nodes representing web-page impressions on the other; real-world instances of such graphs can have billions of impression nodes. We provide theoretical guarantees for our algorithms, and in a real-world advertising application, we demonstrate the feasibility of our detection algorithms. :::</li>
<li><strong>Monotonicity in Bargaining Networks</strong> [<a href="pubs/monotonicity.pdf">pdf</a> | <a href="#MON">Show/Hide Abstract</a>]<br />
with Yossi Azar, Kamal Jain and Yuval Rabani. In Proc. <em>SODA 2010</em> ::: {#absMON style=“display:none”} We study bargaining networks, discussed in a recent paper of Kleinberg and Tardos, from the perspective of cooperative game theory. In particular we examine three solution concepts, the nucleolus, the core center and the core median. All solution concepts define unique solutions, so they provide testable predictions. We define a new monotonicity property that is a natural axiom of any bargaining game solution, and we prove that all three of them satisfy this monotonicity property. This is actually in contrast to the conventional wisdom for general cooperative games that monotonicity and the core condition (which is a basic property that all three of them satisfy) are incompatible with each other. Our proofs are based on a primal-dual argument (for the nucleolus) and on the FKG inequality (for the core center and the core median). We further observe some qualitative differences between the solution concepts. In particular, there are cases where a strict version of our monotonicity property is a natural axiom, but only the core center and the core median satisfy it. On the other hand, the nucleolus is easy to compute, whereas computing the core center or the core median is #P-hard (yet it can be approximated in polynomial time). :::</li>
</ul>
<p><strong>2009</strong></p>
<ul>
<li><strong>Convergence of Local Dynamics to Balanced Outcomes in Exchange Networks</strong> [<a href="pubs/edgebalancing.pdf">pdf</a> | <a href="#EB">Show/Hide Abstract</a>]<br />
with Yossi Azar, Benjamin Birnbaum, L. Elisa Celis and Yuval Peres. In Proc. <em>FOCS 2009</em> ::: {#absEB style=“display:none”} Bargaining games on exchange networks have been studied by both economists and sociologists. A Balanced Out- come for such a game is an equilibrium concept that combines notions of stability and fairness. In a recent paper, Kleinberg and Tardos introduced balanced outcomes to the computer science community and provided a polynomial-time algorithm to compute the set of such outcomes. Their work left open a pertinent question: are there natural, local dynamics that converge quickly to a balanced outcome? In this paper, we provide a partial answer to this question by showing that simple edge- balancing dynamics converge to a balanced outcome whenever one exists. :::</li>
<li><strong>The Price of Truthfulness for Pay-Per-Click Auctions</strong> [<a href="pubs/ppc.pdf">pdf</a> | <a href="#PPC">Show/Hide Abstract</a>]<br />
with Sham Kakade. In Proc. <em>ACM EC 2009.</em> ::: {#absPPC style=“display:none”} We analyze the problem of designing a truthful pay-per-click auction where the click-through-rates (CTR) of the bidders are unknown to the auction. Such an auction faces the classic explore/exploit dilemma: while gathering information about the click through rates of advertisers, the mechanism may loose revenue; however, this gleaned information may prove valuable in the future for a more profitable allocation. In this sense, such mechanisms are prime candidates to be designed using multi-armed bandit techniques. However, a naive application of multi-armed bandit algorithms would not take into account the strategic considerations of the players --- players might manipulate their bids (which determine the auction's revenue) in a way as to maximize their own utility. Hence, we consider the natural restriction that the auction be truthful. The revenue that we could hope to achieve is the expected revenue of a Vickrey auction that knows the true CTRs, and we define the truthful regret to be the difference between the expected revenue of the auction and this Vickrey revenue. This work sharply characterizes what regret is achievable, under a truthful restriction. We show that this truthful restriction imposes statistical limits on the achievable regret --- the achievable regret is $\tilde{\Theta}(T^{2/3})$, while for traditional bandit algorithms (without the truthful restriction) the achievable regret is $\tilde{\Theta}(T^{1/2})$ (where $T$ is the number of rounds). We term the extra $T^{1/6}$ factor, the `price of truthfulness'. :::</li>
<li><strong>The Adwords Problem: Online Keyword Matching with Budgeted Bidders under Random Permutations</strong> [<a href="pubs/Adwords.pdf">pdf</a> | <a href="#ADW">Show/Hide Abstract</a>]<br />
with Tom Hayes. In Proc. <em>ACM EC 2009.</em> ::: {#absADW style=“display:none”} We consider the problem of a search engine trying to assign a sequence of search keywords to a set of competing bidders, each with a daily spending limit. The goal is to maximize the revenue generated by these keyword sales, bearing in mind that, as some bidders may eventually exceed their budget, not all keywords should be sold to the highest bidder. We assume that the sequence of keywords (or equivalently, of bids) is revealed on-line. Our concern will be the competitive ratio for this problem versus the off-line optimum. We extend the current literature on this problem by considering the setting where the keywords arrive in a random order. In this setting we are able to achieve a competitive ratio of $1-\epsilon$ under some mild, but necessary, assumptions. In contrast, it is already known that when the keywords arrive in an adversarial order, the best competitive ratio is bounded away from
<ol type="1">
<li>Our algorithm is motivated by PAC learning, and proceeds in two parts: a training phase, and an exploitation phase. :::</li>
</ol></li>
<li><strong>Limited and Online Supply and the Bayesian foundations of prior-free mechanism design</strong> [<a href="pubs/los.pdf">pdf</a> | <a href="#LOS">Show/Hide Abstract</a>]<br />
with Jason Hartline. In Proc. <em>ACM EC 2009.</em> ::: {#absLOS style=“display:none”} We study auctions for selling a limited supply of a single commodity in the case where the supply is known in advance and the case it is unknown and must be instead allocated in an online fashion. The latter variant was proposed by Mahdian and Saberi as a model of an important phenomena in auctions for selling Internet advertising: advertising impressions must be allocated as they arrive and the total quantity available is unknown in advance. We describe the Bayesian optimal mechanism for these variants and extend the random sampling auction of Goldberg et al. to address the prior-free case. :::</li>
</ul>
<div data-align="left">
<p>– –</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span id="pub">Other Publications</span></td>
</tr>
</tbody>
</table>
<ul>
<li><p><strong>A Unified Rounding Algorithm For Unrelated Machines Scheduling Problems.</strong><br />
with Janardhan Kulkarni. In Proc. <em>SPAA 2018</em>. ::: {#absURA style=“display:none”}</p>
<p>:::</p></li>
<li><p><strong>A Rational Convex Program for Linear Arrow-Debreu Markets</strong> [<a href="http://arxiv.org/pdf/1307.8037.pdf">pdf</a>| <a href="#RCP">Show/Hide Abstract</a>]<br />
with Jugal Garg and Laszlo A. Vegh . In <em>TEAC,</em> Volume 5 Issue 1, November 2016. ::: {#absRCP style=“display:none”} We give a new, flow-type convex program describing equilibrium solutions to linear Arrow-Debreu markets. Whereas convex formulations were previously known [Nenakov, Primak 83; Jain 07; Cornet '89], our program exhibits several new features. It gives a simple necessary and sufficient condition and a concise proof of the existence and rationality of equilibria, settling an open question raised by Vazirani. As a consequence we also obtain a simple new proof of Mertens's result that the equilibrium prices form a convex polyhedral set.</p>
<p>:::</p></li>
<li><p><strong>On the Approximation of Submodular Functions</strong> [<a href="http://arxiv.org/pdf/1307.8037.pdf">pdf</a>| <a href="#ASM">Show/Hide Abstract</a>]<br />
with Shaddin Dughmi, Roy Schwartz, Ankit Sharma and Mohit Singh. ::: {#absASM style=“display:none”} Submodular functions are a fundamental object of study in combinatorial optimization, economics, machine learning, etc. and exhibit a rich combinatorial structure. Many subclasses of submodular functions have also been well studied and these subclasses widely vary in their complexity. Our motivation is to understand the relative complexity of these classes of functions. Towards this, we consider the question of how well can one class of submodular functions be approximated by another (simpler) class of submodular functions. Such approximations naturally allow algorithms designed for the simpler class to be applied to the bigger class of functions. We prove both upper and lower bounds on such approximations.<br />
Our main results are:<br />
</p>
<ol type="1">
<li>General submodular functions can be approximated by cut functions of directed graphs to a factor of $n^2/4$, which is tight.<br />
</li>
<li>General symmetric submodular functions$^{1}$ can be approximated by cut functions of undirected graphs to a factor of $n-1$, which is tight up to a constant.<br />
</li>
<li>Budgeted additive functions can be approximated by coverage functions to a factor of $e/(e-1)$, which is tight.<br />
Here $n$ is the size of the ground set on which the submodular function is defined.<br />
We also observe that prior works imply that monotone submodular functions can be approximated by coverage functions with a factor between $O(\sqrt{n} \log n)$ and $\Omega(n^{1/3} /\log^2 n) $.</li>
</ol>
<p>:::</p></li>
<li><p><strong>Sequential Auctions of Identical Items with Budget-Constrained Bidders</strong> [<a href="http://arxiv.org/pdf/1209.1698.pdf">pdf</a>| <a href="#SAB">Show/Hide Abstract</a>]<br />
with Zhiyi Huang and David Malec. ::: {#absSAB style=“display:none”} In this paper, we study sequential auctions with two budget constrained bidders and any number of identical items. All prior results on such auctions consider only two items. We construct a canonical outcome of the auction that is the only natural equilibrium and is unique under a refinement of subgame perfect equilibria. We show certain interesting properties of this equilibrium; for instance, we show that the prices decrease as the auction progresses. This phenomenon has been observed in many experiments and previous theoretic work attributed it to features such as uncertainty in the supply or risk averse bidders. We show that such features are not needed for this phenomenon and that it arises purely from the most essential features: budget constraints and the sequential nature of the auction. A little surprisingly we also show that in this equilibrium one agent wins all his items in the beginning and then the other agent wins the rest. The major difficulty in analyzing such sequential auctions has been in understanding how the selling prices of the first few rounds affect the utilities of the agents in the later rounds. We tackle this difficulty by identifying certain key properties of the auction and the proof is via a joint induction on all of them.</p>
<p>:::</p></li>
<li><p><strong>Cloud Scheduling with Setup Cost</strong> [<a href="pubs/schedule6.pdf">pdf</a>| <a href="#PR">Show/Hide Abstract</a>]<br />
with Yossi Azar, Naama Ben-Aroya and Navendu Jain. In Proc. <em>SPAA 2013</em> ::: {#absCSSC style=“display:none”} In this paper, we investigate the problem of online task scheduling of jobs such as MapReduce jobs, Monte Carlo simulations and generating search index from web documents, on cloud computing infrastructures. We consider the virtualized cloud computing setup comprising machines that host multiple identical virtual machines (VMs) under pay-as-you-go charging, and that booting a VM requires a constant setup time. The cost of job computation depends on the number of VMs activated, and the VMs can be activated and shutdown on demand. We propose a new bi-objective algorithm to minimize the maximum task delay, and the total cost of the computation. We study both the clairvoyant case, where the duration of each task is known upon its arrival, and the more realistic non-clairvoyant case.</p>
<p>:::</p></li>
<li><p><strong>Prior-Independent Multi-parameter Mechanism Design</strong> [<a href="pubs/udca.pdf">pdf</a> | <a href="#PR">Show/Hide Abstract</a>]<br />
with Jason D. Hartline, Anna R. Karlin, C. Thach Nguyen In Proc. <em>WINE 2011</em> ::: {#absPIMD style=“display:none”} In a multi-unit unit-demand multi-item auction, an auctioneer is selling a collection of different items to a set of agents each interested in buying at most unit. Each agent has a different private value for each of the items. We consider the problem of designing a truthful auction that maximizes the auctioneer's profit in this setting. Previously, there has been progress on this problem in the setting in which each value is drawn from a known prior distribution. Specifically, it has been shown how to design auctions tailored to these priors that achieve a constant factor approximation ratio profit. In this paper, we present the first prior-independent auction for this setting. This auction is guaranteed to achieve a constant fraction of the optimal expected profit for a large class of, so called, ``regular'' distributions, without specific knowledge of the distributions.</p>
<p>:::</p></li>
<li><p><strong>An O(n log n) Algorithm for a Load Balancing Problem on Paths</strong> [<a href="pubs/PathBalancing.pdf">pdf</a>| <a href="#PR">Show/Hide Abstract</a>]<br />
with Uri Feige. In Proc. <em>WADS 2011</em> ::: {#absPB style=“display:none”} We study the following load balancing problem on paths (PB). There is a path containing n vertices. Every vertex i has an initial load h i , and every edge (j, j + 1) has an initial load w j that it needs to distribute among the two vertices that are its endpoints. The goal is to distribute the load of the edges over the vertices in a way that will make the loads of the vertices as balanced as possible (formally, mini- mizing the sum of squares of loads of the vertices). This problem can be solved in polynomial time, e.g, by dynamic programming. We present an algorithm that solves this problem in time O(n log n). As a mental aide in the design of our algorithm, we ﬁrst design a hy- draulic apparatus composed of bins (representing vertices), tubes (rep- resenting edges) that are connected between bins, cylinders within the tubes that constrain the ﬂow of water, and valves that can close the con- nections between bins and tubes. Water may be poured into the various bins, to levels that correspond to the initial loads in the input to the PB problem. When all valves are opened, the water ﬂows between bins (to the extent that is feasible due to the cylinders) and stabilizes at levels that are the correct output to the respective PB problem. Our algorithm is based on a fast simulation of the behavior of this hydraulic apparatus, when valves are opened one by one.</p>
<p>:::</p></li>
<li><p><strong>Local Dynamics in Bargaining Networks via Random-Turn Games</strong> [<a href="pubs/BargainingRTG.pdf">pdf</a>| <a href="#PR">Show/Hide Abstract</a>]<br />
with Elisa Celis and Yuval Peres. In Proc. <em>WINE 2010</em> ::: {#absRTG style=“display:none”} We present a new technique for analyzing the rate of convergence of local dynamics in bargaining networks. The technique reduces balancing in a bargaining network to optimal play in a random-turn game. We analyze this game using techniques from martingale and Markov chain theory. We obtain a tight polynomial bound on the rate of convergence for a nontrivial class of unweighted graphs (the previous known bound was exponential). Additionally, we show this technique extends naturally to many other graphs and dynamics.</p>
<p>:::</p></li>
<li><p><strong>Market Equilibrium with Transaction Costs</strong> [<a href="pubs/mewtc.pdf">pdf</a>| <a href="#PR">Show/Hide Abstract</a>]<br />
with Sourav Chakraborty and Chinmay Karande. In Proc. <em>WINE 2010</em> ::: {#absMEWTC style=“display:none”} Identical products being sold at different prices in different locations is a common phenomenon. To model such scenarios, we supplement the classical Fisher market model by introducing {\em transaction costs}. For every buyer $i$ and good $j$, there is a transaction cost of $c_{ij}$; if the price of good $j$ is $p_j$, then the cost to the buyer $i$ {\em per unit} of $j$ is $p_j + c_{ij}$. The same good can thus be sold at different (effective) prices to different buyers. We provide a combinatorial algorithm that computes $\epsilon$-approximate equilibrium prices and allocations in $O\left(\frac{1}{\epsilon}(n+\log{m})mn\log(B/\epsilon)\right)$ operations - where $m$ is the number goods, $n$ is the number of buyers and $B$ is the sum of the budgets of all the buyers.</p>
<p>:::</p></li>
<li><p><strong>An Online Multi-unit Auction with Improved Competitive Ratio</strong> [<a href="pubs/OMUA.pdf">pdf</a> | <a href="#OMUA">Show/Hide Abstract</a>]<br />
with Sourav Chakraborty. In Proc. <em>WINE 2009.</em> ::: {#absOMUA style=“display:none”} We improve the best known competitive ratio (from 1/4 to 1/2), for the online multi-unit allocation problem, where the objective is to maximize the single-price revenue. Moreover, the competitive ratio of our algorithm tends to 1, as the bid-profile tends to "smoothen". This algorithm is used as a subroutine in designing truthful auctions for the same setting: the allocation has to be done online, while the payments can be decided at the end of the day. Earlier, a reduction from the auction design problem to the allocation problem was known only for the unit-demand case. We give a reduction for the general case when the bidders have decreasing marginal utilities. The problem is inspired by sponsored search auctions.</p>
<p>:::</p></li>
<li><p><strong>A Computational Theory of Awareness and Decision Making</strong> [<a href="pubs/awareness.pdf">pdf</a> | <a href="#AWA">Show/Hide Abstract</a>]<br />
with Lance Fortnow. In Proc. <em>Theoretical Aspects of Rationality and Knowledge, TARK 2009</em> ::: {#absAWA style=“display:none”} We exhibit a new computational-based definition of awareness, informally that our level of unawareness of an object is the amount of time needed to generate that object within a certain environment. We give several examples to show this notion matches our intuition in scenarios where one organizes, accesses and transfers information. We also give a formal process-independent definition of awareness based on Levin's universal enumeration. We show the usefulness of computational awareness by showing how it relates to decision making, and how others can manipulate our decision making with appropriate advertising, in particular, we show connections to sponsored search and brand awareness. Understanding awareness can also help rate the effectiveness of various user interfaces designed to access information.</p>
<p>:::</p></li>
<li><p><strong>Market Equilibria in Polynomial time for fixed number of goods or agents</strong> [<a href="pubs/PLC.pdf">pdf</a> | <a href="#PLC">Show/Hide Abstract</a>]<br />
with Ravi Kannan. In Proc. <em>FOCS 2008.</em> ::: {#absPLC style=“display:none”} We consider markets in the classical Arrow-Debreu model. There are n agents and m goods. Each buyer has a concave utility function (of the bundle of goods he/she buys) and an initial bundle. At an ``equilibrium'' set of prices for goods, if each individual buyer separately exchanges the initial bundle for an optimal bundle at the set prices, the market clears, i.e., all goods are exactly consumed. Classical theorems guarantee the existence of equilibria, but computing them has been the subject of much recent research. In the related area of Multi-Agent Games, much attention has been paid to the complexity as well as algorithms. While most general problems are hard, polynomial time algorithms have been developed for restricted classes of games, when one assumes the number of strategies is constant.</p>
<p>For the Market Equilibrium problem, several important special cases of utility functions have been tackled. Here we begin a program for this problem similar to that for multi-agent games, where general utilities are considered. We begin by showing that if the utilities are separable piece-wise linear concave (PLC) functions, and the number of goods (or alternatively the number of buyers) is constant, then we can compute an exact equilibrium in polynomial time. Our technique for the constant number of goods is to decompose the space of price vectors into cells using certain hyperplanes, so that in each cell, each buyer's threshold marginal utility is known. Still, one needs to solve a linear optimization problem in each cell. We then show the main result - that for general (non-separable) PLC utilities, an exact equilibrium can be found in polynomial time provided the number of goods is constant. The starting point of the algorithm is a ``cell-decomposition'' of the space of price vectors using polynomial surfaces (instead of hyperplanes). We use results from computational algebraic geometry to bound the number of such cells. For solving the problem inside each cell, we introduce and use a novel LP-duality based method. We note that if the number of buyers and agents both can vary, the problem is PPAD hard even for the very special case of PLC utilities such as Leontief utilities and separable PLC utilities.</p>
<p>:::</p></li>
<li><p><strong>New Geometry-Inspired Relaxations and Algorithms for the Metric Steiner Tree Problem.</strong> [<a href="pubs/steiner.pdf">pdf</a> | <a href="#STI">Show/Hide Abstract</a>]<br />
with Deeparnab Chakrabarty, and Vijay Vazirani. In <em>Math Programming (Series A)</em> Volume 122, Number 2 (April 2010). (Preliminary version in <em>IPCO 2008.</em> ) ::: {#absSTI style=“display:none”} Determining the integrality gap of the bidirected cut relaxation for the metric Steiner tree problem, and exploiting it algorithmically, is a long-standing open problem. We use geometry to define an LP whose dual is equivalent to this relaxation. This opens up the possibility of using the primal-dual schema in a geometric setting for designing an algorithm for this problem. Using this approach, we obtain a 4/3 factor algorithm and integrality gap bound for the case of quasi-bipartite graphs; the previous best integrality gap upper bound being 3/2. We also obtain a factor $sqrt{2}$ strongly polynomial algorithm for this class of graphs. A key difficulty experienced by researchers in working with the bidirected cut relaxation was that any reasonable dual growth procedure produces extremely unwieldy dual solutions. A new algorithmic idea helps finesse this difficulty - that of reducing the cost of certain edges and constructing the dual in this altered instance - and this idea can be extracted into a new technique for running the primal-dual schema in the setting of approximation algorithms.</p>
<p>:::</p></li>
<li><p><strong>On Computing the Distinguishing Numbers of Planar Graphs and Beyond: a Counting Approach</strong> [<a href="pubs/dist.pdf">pdf</a> | <a href="#DIST">Show/Hide Abstract</a>]<br />
with V. Arvind and Christine Cheng. In <em>SIAM Journal on Discrete Mathematics,</em> vol. 22, no. 4 (October 2008), pp. 1297-1324. ::: {#absDIST style=“display:none”} A vertex k-labeling of graph G is distinguishing if the only automorphism that preserves the labels of G is the identity map. The distinguishing number of G, D(G), is the smallest integer k for which G has a distinguishing k-labeling. In this paper, we apply the principle of inclusion-exclusion and develop recursive formulas to count the number of inequivalent distinguishing k-labelings of a graph. Along the way, we prove that the distinguishing number of a planar graph can be computed in time polynomial in the size of the graph.</p>
<p>:::</p></li>
<li><p><strong>On the Equivalence of Competitive and Submodular markets</strong> [<a href="pubs/competitive.pdf">pdf</a> | <a href="#COM">Show/Hide Abstract</a>]<br />
with Deeparnab Chakrabarty. In <em>Operations Research Letters,</em> vol. 37, no. 3, pp. 155 - 158, 2009. Prelim. version in Proc. <em>WINE 2007</em> ::: {#absCOM style=“display:none”} In this paper, we study competitive markets - a market is competitive if increasing the endowment of any one buyer does not in- crease the equilibrium utility of any other buyer. In the Fisher setting, competitive markets contain all markets with weak gross substitutabil- ity (WGS), a property which enable efficient algorithms for equilibrium computation. We show that every uniform utility allocation (UUA) market which is competitive, is a submodular utility allocation (SUA) market. Our result provides evidence for the existence of efficient algoritheorems for the class of competitive markets.</p>
<p>:::</p></li>
<li><p><strong>Computing Market Equilibrium: Beyond Weak Gross Substitutes</strong> [<a href="pubs/ApproxWGS.pdf">pdf</a> | <a href="#AWGS">Show/Hide Abstract</a>]<br />
with Chinmay Karande. In Proc. <em>WINE 2007</em> ::: {#absAWGS style=“display:none”} The property of Weak Gross Substitutibility (WGS) of goods in a market has been found to be conducive to efficient algorithms for finding equilibria. In this paper, we give a natural definition of a $\delta$ approximate WGS property, and show that the auction algorithm of Garg and Kapoor can be extended to give an $\epsilon + \delta$ approximate equilibrium for markets with this property.</p>
<p>:::</p></li>
<li><p><strong>New results on Rationality and Strongly Poylnomial Solvability in Eisenberg-Gale markets</strong> [<a href="pubs/EG2.pdf">pdf</a> | <a href="#EG2">Show/Hide Abstract</a>]<br />
with Deeparnab Chakrabarty and Vijay Vazirani. In Proc. <em>WINE 2006</em> ::: {#absEG2 style=“display:none”} We study the structure of EG(2) markets, the class of Eisenberg-Gale markets with two agents. We prove that all markets in this class are rational and they admit strongly polynomial algorithms whenever the polytope containing the set of feasible utilities of the two agents can be described via a combinatorial LP. This helps resolve positively the status of two markets left as open problems by Jain and Vazirani: the capacity allocation market in a directed graph with two source-sink pairs and the network coding market in a directed network with two sources. Our algorithms for solving the corresponding nonlinear convex programs are fundamentally different from those obtained by Jain and Vazirani; whereas they use the primal- dual schema, we use a carefully constructed binary search.</p>
<p>:::</p></li>
<li><p><strong>Integrality Gaps for Sparsest Cut and Minimum Linear Arrangement Problems</strong> [<a href="pubs/usc.pdf">pdf</a> | <a href="#USC">Show/Hide Abstract</a>]<br />
with Subhash A. Khot, Rishi Saket and Nisheeth K. Vishnoi. In Proc. <em>STOC 2006.</em> ::: {#absUSC style=“display:none”} Arora, Rao and Vazirani showed that the standard semi-definite programming (SDP) relaxation of the sparsest cut problem with the triangle inequality constraints has an integrality gap of $O(\sqrt{\log n})$. They conjectured that the gap is bounded from above by a constant. In this paper, we disprove this conjecture by constructing an $\Omega(\log \log n)$ integrality gap instance. Khot and Vishnoi had earlier disproved the non-uniform version of this Conjecture. A simple ``stretching'' of the integrality gap instance for the sparsest cut problem serves as an $\Omega(\log \log n)$ integrality gap instance for the SDP relaxation of the Minimum Linear Arrangement problem. This SDP relaxation was considered in Charikar et. al. and Feige and Lee, where it was shown that its integrality gap is bounded from above by $O(\sqrt{\log n} \log \log n).$</p>
<p>:::</p></li>
<li><p><strong>Price of Anarchy, Locality Gap, and a Network Service Provider Game</strong> [<a href="pubs/POA.pdf">pdf</a> | <a href="#POA">Show/Hide Abstract</a>]<br />
with Naveen Garg, Rohit Khandekar, Vinayaka Pandit, Amin Saberi, and Vijay V. Vazirani. In Proc. <em>WINE 2005</em> ::: {#absPOA style=“display:none”} In this paper, we define a network service provider game. We show that the price of anarchy of the defined game can be bounded by analyzing a local search heuristic for a related facility location problem called the $k$-facility location problem. As a result, we show that the $k$-facility location problem has a locality gap of 5. This result is of interest on its own. Our result gives evidence to the belief that the price of anarchy of certain games are related to analysis of local search heuristics.</p>
<p>:::</p></li>
<li><p><strong>On the complexity of Hilbert's 17th problem</strong> [<a href="pubs/hilbert.pdf">pdf</a> | <a href="#HIL">Show/Hide Abstract</a>]<br />
with Richard J. Lipton and Nisheeth Vishnoi. In Proc. <em>FSTTCS 2004.</em> ::: {#absHIL style=“display:none”} Hilbert posed the following problem as the 17th in the list of 23 problems in his famous 1900 lecture: <em>Given a multivariate polynomial that takes only non-negative values over the reals, can it be represented as a sum of squares of rational functions?</em> In 1927, E.~Artin gave an affirmative answer to this question. His result guaranteed the existence of such a finite representation and raised the following important question: <em>What is the {\bf minimum number} of rational functions needed to represent any non-negative $n$-variate, degree $d$ polynomial?</em> In 1967, Pfister proved that any $n$-variate non-negative polynomial over the reals can be written as sum of squares of at most $2^n$ rational functions. In spite of a considerable effort by mathematicians for over 75 years, it is <em>not</em> known whether $n+2$ rational functions are sufficient! In lieu of the lack of progress towards the resolution of this question, we initiate the study of Hilbert's 17th problem from the point of view of Computational Complexity. In this setting, the following question is a natural relaxation: <em>What is the {\bf descriptive complexity} of the sum of squares representation (as rational functions) of a non-negative, $n$-variate, degree $d$ polynomial?</em> We consider arithmetic circuits as a natural representation of rational functions. We are able to show, assuming a standard conjecture in complexity theory, that it is impossible that every non-negative, $n$-variate, degree four polynomial can be represented as a sum of squares of a small (polynomial in $n$) number of rational functions, each of which has a small size arithmetic circuit (over the rationals) computing it.</p>
<p>Our result points to the direction that it is unlikely that every non-negative, $n$-variate polynomial over the reals can be written as a sum of squares of a polynomial (in $n$) number of rational functions. Further, relating to standard (and believed to be hard to prove) complexity-theoretic conjectures sheds some light on why it has been difficult for mathematicians to close the $n+2$ and $2^n$ gap. We hope that our line of work will play an important role in the resolution of this question.</p>
<p>:::</p></li>
<li><p><strong>The Spending Constraint Model for Market Equilibrium: Algorithmic, Existence and Uniqueness results</strong> [<a href="pubs/sconstraint.pdf">pdf</a> | <a href="#SC">Show/Hide Abstract</a>]<br />
with Vijay V. Vazirani. In Proc. <em>STOC 2004.</em> ::: {#absSC style=“display:none”} The traditional model of market equilibrium supports impressive existence results, including the celebrated Arrow-Debreu Theorem. However, in this model, polynomial time algorithms for computing (or approximating) equilibria are known only for linear utility functions. We present a new, and natural, model of market equilibrium that not only admits existence and uniqueness results paralleling those for the traditional model but is also amenable to efficient algorithms.</p>
<p>:::</p></li>
<li><p><strong>An Improved Approximation Scheme for Computing Arrow-Debreu Prices for the Linear Case</strong> [<a href="pubs/adptas.pdf">pdf</a> | <a href="#AD">Show/Hide Abstract</a>]<br />
with Vijay Vazirani. In Proc. <em>FSTTCS 2003.</em> ::: {#absAD style=“display:none”} Recently, Jain, Mahdian and Saberi had given a FPTAS for the problem of computing a market equilibrium in the Arrow-Debreu setting, when the utilities are linear functions. Their running time depended on the size of the numbers representing the utilities and endowments of the buyers. In this paper, we give a strongly polynomial time approximation scheme for this problem. Our algorithm builds upon the main ideas behind the algorithm in Devanur et. al.</p>
<p>:::</p></li>
<li><p><strong>Strategyproof cost-sharing Mechanisms for Set Cover and Facility Location Games</strong> [<a href="pubs/DMV.pdf">pdf</a> | <a href="#DMV">Show/Hide Abstract</a>]<br />
with Milena Mihail and Vijay Vazirani. <em>Decision Support Systems</em> 39 (March 2005), pp 11--22. Prelim. version in Proc. <em>ACM EC 2003</em> ::: {#absDMV style=“display:none”} Strategyproof cost-sharing mechanisms, lying in the core, that recover $1/\alpha$ fraction of the cost, are presented for the set cover and facility location games; $\alpha = O(\log n)$ for the former and 1.861 for the latter. Our mechanisms utilize approximation algorithms for these problems based on the method of dual-fitting.</p>
<p>:::</p></li>
<li><p><strong>Market Equilibrium via a Primal-Dual-Type Algorithm</strong> [<a href="pubs/DPSV-JACM.pdf">pdf</a> | <a href="#DPSV">Show/Hide Abstract</a>]<br />
with Christos H. Papadimitriou, Amin Saberi and Vijay V.Vazirani. In The <em>Journal of the ACM</em>, vol. 55, no. 5 (October 2008), pp 1--18. Prelim version appeared in <em>FOCS 2002.</em> ::: {#absDPSV style=“display:none”} We give the first polynomial time algorithm for exactly computing an equilibrium for the linear utilities case of the market model defined by Fisher. Our algorithm uses the primal-dual paradigm in the enhanced setting of KKT conditions and convex programs. We pinpoint the added difficulty raised by this setting and the manner in which our algorithm circumvents it.</p>
<p>:::</p></li>
</ul>
</div>
